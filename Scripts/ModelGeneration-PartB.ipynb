{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf1550f-0054-4b28-989a-cf9eafd85ec7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98c6b3-b259-4a1a-b73f-5fea16722f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas==2.0.3\n",
    "%pip install tqdm==4.66.1\n",
    "%pip install pm4py==2.7.8.2\n",
    "%pip install pygraphviz==1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8d56b2a06a0dca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:06.407815Z",
     "start_time": "2023-11-08T23:49:59.832275Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from enum import Enum, auto\n",
    "from IPython.display import display, HTML\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "from pm4py.objects.bpmn.obj import BPMN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d8160e-c7ef-4c76-aaa5-ed7d2a66a6b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Process Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0500daa-6b2d-4148-b5ff-6f66e67240f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:07.696172Z",
     "start_time": "2023-11-08T23:50:06.428987Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color: green;'>All logging_statement_ids accounted for</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style='color: green;'>All logging_statement_ids accounted for</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style='color: green;'>All logging_statement_ids accounted for</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style='color: green;'>All logging_statement_ids accounted for</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><span style='color: blue;'>&emsp;&emsp; --> DONE</span></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ProcessingMode(Enum):\n",
    "    MODEL = auto()\n",
    "\n",
    "def discover_bpmn_graph(log_data, graph_prefix):        \n",
    "    \"\"\"Extract Petri net graph from log data.\"\"\"\n",
    "    logs_df = pd.DataFrame(log_data)   \n",
    "    logs_df['timestamp'] = pd.to_datetime(logs_df['timestamp'])\n",
    "    logs_df['logging_statement_id'] = logs_df['logging_statement_id'].astype(str)\n",
    "    logs_df['case_id'] = '01'\n",
    "    \n",
    "    params = {\n",
    "        'dependency_threshold': 0.45,\n",
    "        'and_threshold': 0.65,\n",
    "        'loop_two_threshold': 0.5,\n",
    "        'activity_key': 'logging_statement_id',\n",
    "        'case_id_key': 'case_id',\n",
    "        'timestamp_key': 'timestamp'\n",
    "    }\n",
    "    net, im, fm = pm4py.discover_petri_net_heuristics(logs_df, **params)\n",
    "    \n",
    "    bpmn_graph = pm4py.convert_to_bpmn(net, im, fm)\n",
    "    \n",
    "    # Get all unique logging_statement_ids from DataFrame\n",
    "    unique_logging_statement_ids = set(logs_df['logging_statement_id'].unique())\n",
    "\n",
    "    # Check if all unique_logging_statement_ids are in the tasks of the BPMN graph\n",
    "    bpmn_tasks = [node for node in bpmn_graph.get_nodes() if isinstance(node, BPMN.Task)]\n",
    "    task_labels = set(task.get_name() for task in bpmn_tasks)\n",
    "\n",
    "    missing_ids = unique_logging_statement_ids - task_labels\n",
    "\n",
    "    if missing_ids:\n",
    "        missing_ids_str = '<br>'.join(missing_ids)\n",
    "        display(HTML(f\"<span style='color: red;'>Missing logging_statement_ids in BPMN graph:<br>{missing_ids_str}</span>\"))\n",
    "    else:\n",
    "        display(HTML(f\"<span style='color: green;'>All logging_statement_ids accounted for</span>\"))\n",
    "            \n",
    "    bpmn_graph = pm4py.convert_to_bpmn(net, im, fm)\n",
    "    \n",
    "    return bpmn_graph\n",
    "\n",
    "def load_logs(base_dir, mode):\n",
    "    \"\"\"Load logs from a file.\"\"\"\n",
    "    with open(f'{base_dir}/processed_logs_mode_{mode.name}.json') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_unique_services_and_subprocesses(logs_A, logs_B):\n",
    "    \"\"\"Get unique services and subprocesses between two sets of logs.\"\"\"\n",
    "    unique_services = set(logs_A.keys()).union(set(logs_B.keys()))\n",
    "    unique_subprocesses = {}\n",
    "    for service in unique_services:\n",
    "        subprocesses_A = set(logs_A.get(service, {}).keys())\n",
    "        subprocesses_B = set(logs_B.get(service, {}).keys())\n",
    "        unique_subprocesses[service] = subprocesses_A.union(subprocesses_B)\n",
    "    return unique_services, unique_subprocesses\n",
    "\n",
    "def create_empty_bpmn():\n",
    "    \"\"\"Create an empty BPMN graph with a single start event.\"\"\"\n",
    "    empty_bpmn = BPMN(name='Empty BPMN')\n",
    "    return empty_bpmn\n",
    "\n",
    "def process_graphs_and_save(logs, graph_name, all_process_graphs, unique_services, unique_subprocesses):\n",
    "    \"\"\"Generate process graphs and save them.\"\"\"\n",
    "    for mode in ProcessingMode:\n",
    "        all_process_graphs.setdefault(graph_name, {}).setdefault(mode.name, {})\n",
    "        for service in unique_services[mode]:\n",
    "            for subprocess in unique_subprocesses[mode].get(service, {}):\n",
    "                target_logs = logs[mode.name].get(service, {}).get(subprocess, {}).get('logs', [])\n",
    "                \n",
    "                 # Check if target_logs is empty\n",
    "                if not target_logs:\n",
    "                    print(f\"No logs found for {service} -> {subprocess} in {mode.name}. Creating an empty BPMN graph.\")\n",
    "                    bpmn_graph = create_empty_bpmn()\n",
    "                else:\n",
    "                    graph_prefix = f'{graph_name.split()[-1]}'\n",
    "                    bpmn_graph = discover_bpmn_graph(target_logs, graph_prefix)\n",
    "                \n",
    "                # Save net along with its initial and final markings\n",
    "                all_process_graphs[graph_name][mode.name].setdefault(service, {})[subprocess] = {\n",
    "                    'bpmn_graph': bpmn_graph,\n",
    "                }\n",
    "\n",
    "# Load logs\n",
    "logs_A = {mode.name: load_logs('../Data/Reproduced Experiment/Preprocessed/Logs/Part B/Expected', mode) for mode in ProcessingMode}\n",
    "logs_B = {mode.name: load_logs('../Data/Reproduced Experiment/Preprocessed/Logs/Part B/Faulty', mode) for mode in ProcessingMode}\n",
    "\n",
    "# Get unique services and subprocesses for each mode\n",
    "unique_services = {}\n",
    "unique_subprocesses = {}\n",
    "for mode in ProcessingMode:\n",
    "    unique_services[mode], unique_subprocesses[mode] = get_unique_services_and_subprocesses(logs_A[mode.name], logs_B[mode.name])\n",
    "\n",
    "# Initialize all_process_s dictionary\n",
    "all_process_graphs = {}\n",
    "\n",
    "# Process graphs and save them, also fill in for missing services and subprocesses\n",
    "process_graphs_and_save(logs_A, 'Expected', all_process_graphs, unique_services, unique_subprocesses)\n",
    "process_graphs_and_save(logs_B, 'Faulty', all_process_graphs, unique_services, unique_subprocesses)\n",
    "\n",
    "display(HTML(f\"<b><span style='color: blue;'>&emsp;&emsp; --> DONE</span></b>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f386f5b-9c33-4abd-93e3-57058f2079b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gateway Identification Using Path-Based Signiture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38fdf07b-6c36-4dda-8378-ba72e640ea64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:07.955035Z",
     "start_time": "2023-11-08T23:50:07.695595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All gateways from Expected are accounted for in name mappings.\n",
      "All gateways from Faulty are accounted for in name mappings.\n",
      "All gateways from Expected are accounted for in name mappings.\n",
      "All gateways from Faulty are accounted for in name mappings.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def extract_gateways(bpmn_graph):\n",
    "    \"\"\"\n",
    "    This function extracts gateways from a BPMN model and identifies them based on their path-based signatures.\n",
    "    It maps each gateway to the unique paths leading from it, considering only Events and Tasks as path terminators.\n",
    "    The paths are stored as tuples of node names, ensuring a consistent identification system.\n",
    "    This approach allows for the identification of gateways even when only partial information about unique IDs is available,\n",
    "    as it relies on the structural signature of the paths rather than solely on the IDs.\n",
    "    The output is a sorted dictionary where each key represents a unique path structure (as a JSON string),\n",
    "    and the value is a list of gateways sharing that structure. This forms the basis for consistent node identification across different BPMN models.\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries for storing paths by gateway and gateway structures.\n",
    "    paths_by_gateway = {}\n",
    "    structure_to_gateways = {}\n",
    "\n",
    "    # Define a depth-first search (DFS) function to traverse the graph.\n",
    "    def dfs(current_node, current_path, visited_gateways):\n",
    "        # Check if current node is an Event or Task, marking the end of a path.\n",
    "        if isinstance(current_node, (BPMN.Event, BPMN.Task)):\n",
    "            current_path_tuple = tuple(current_path)\n",
    "            paths_by_gateway[start_gateway_id].append(current_path_tuple)\n",
    "            return\n",
    "\n",
    "        # Iterate over outgoing arcs from the current node.\n",
    "        for arc in current_node.get_out_arcs():\n",
    "            next_node = arc.get_target()\n",
    "            # Process next node if it is a Gateway, Event, or Task and not already visited.\n",
    "            if isinstance(next_node, (BPMN.Gateway, BPMN.Event, BPMN.Task)) and next_node not in visited_gateways:\n",
    "                # Get node name, using type name for gateways and node name otherwise.\n",
    "                node_name = type(next_node).__name__ if isinstance(next_node, BPMN.Gateway) else next_node.get_name()\n",
    "                current_path.append(node_name)\n",
    "                # Mark gateway as visited.\n",
    "                if isinstance(next_node, BPMN.Gateway):\n",
    "                    visited_gateways.add(next_node)\n",
    "                # Recursively call DFS on the next node.\n",
    "                dfs(next_node, current_path, visited_gateways)\n",
    "                # Backtrack by removing the last node from the path and unmarking the gateway.\n",
    "                current_path.pop()\n",
    "                if isinstance(next_node, BPMN.Gateway):\n",
    "                    visited_gateways.remove(next_node)\n",
    "\n",
    "    # Start DFS for each gateway node in the BPMN model.\n",
    "    for node in bpmn_graph.get_nodes():\n",
    "        if isinstance(node, BPMN.Gateway):\n",
    "            start_gateway_id = node.id\n",
    "            paths_by_gateway[start_gateway_id] = []\n",
    "            dfs(node, [f\"{type(node).__name__}_Source\"], set())\n",
    "\n",
    "    # Group and sort gateways based on their path structures.\n",
    "    for gateway, paths in paths_by_gateway.items():\n",
    "        # Sort each path\n",
    "        sorted_paths = sorted(paths)\n",
    "        # Convert each sorted path to a string\n",
    "        paths_strings = ['; '.join(path) for path in sorted_paths]\n",
    "        # Convert sorted list of path strings to a JSON string\n",
    "        paths_key = json.dumps(paths_strings)\n",
    "        structure_to_gateways.setdefault(paths_key, []).append(gateway)   \n",
    "    \n",
    "    sorted_structure_to_gateways = {k: structure_to_gateways[k] for k in sorted(structure_to_gateways)}\n",
    "\n",
    "    return sorted_structure_to_gateways\n",
    "        \n",
    "def rebuild_bpmn_graph(bpmn_graph, gateway_mapping):\n",
    "    # Create a new BPMN model with the same name.\n",
    "    new_bpmn_graph = BPMN(name=bpmn_graph.get_name())\n",
    "    node_mapping = {}\n",
    "\n",
    "    # Add nodes to the new model, applying the gateway mapping.\n",
    "    sorted_nodes = sorted(bpmn_graph.get_nodes(), key=lambda node: node.get_name())\n",
    "    for node in sorted_nodes:\n",
    "        new_id = gateway_mapping.get(node.get_id(), node.get_name())\n",
    "        new_node = type(node)(id=new_id, name=new_id, process=node.get_process())\n",
    "        new_bpmn_graph.add_node(new_node)\n",
    "        node_mapping[node] = new_node\n",
    "\n",
    "    # Add flows to the new model, mapping source and target nodes.\n",
    "    sorted_flows = sorted(bpmn_graph.get_flows(), key=lambda flow: (flow.get_source().get_name(), flow.get_target().get_name()))\n",
    "    for flow in sorted_flows:\n",
    "        new_source = node_mapping[flow.get_source()]\n",
    "        new_target = node_mapping[flow.get_target()]\n",
    "        new_flow = type(flow)(source=new_source, target=new_target, id=flow.get_id(), name=flow.get_name(), process=flow.get_process())\n",
    "        new_bpmn_graph.add_flow(new_flow)\n",
    "        \n",
    "    return new_bpmn_graph\n",
    "\n",
    "def handle_gateways_for_model(gateway_paths, other_gateway_paths, gateway_counter, name_mappings, model_label):\n",
    "    for path_structure, gateways in gateway_paths.items():\n",
    "        other_gateways = other_gateway_paths.get(path_structure, [])\n",
    "        num_shared = min(len(gateways), len(other_gateways))\n",
    "        num_unique = len(gateways) - num_shared\n",
    "        # Handle shared gateways\n",
    "        for i in range(num_shared):\n",
    "            count = gateway_counter[\"Shared\"]\n",
    "            new_name = f\"Gateway_{count}_Shared\"\n",
    "            name_mappings[gateways[i]] = new_name\n",
    "            name_mappings[other_gateways[i]] = new_name\n",
    "            gateway_counter[\"Shared\"] += 1\n",
    "        # Handle unique gateways\n",
    "        for i in range(num_shared, num_shared + num_unique):\n",
    "            count = gateway_counter[model_label].get(type(gateways[i]).__name__, 0)\n",
    "            new_name = f\"Gateway_{count}_{model_label}\"\n",
    "            name_mappings[gateways[i]] = new_name\n",
    "            gateway_counter[model_label][type(gateways[i]).__name__] = count + 1\n",
    "            \n",
    "def verify_gateway_mappings(bpmn_model_a, bpmn_model_b, name_mappings):\n",
    "    # Collect IDs of all gateways in the BPMN models A and B\n",
    "    original_gateway_ids_a = {node.id for node in bpmn_model_a.get_nodes() if isinstance(node, BPMN.Gateway)}\n",
    "    original_gateway_ids_b = {node.id for node in bpmn_model_b.get_nodes() if isinstance(node, BPMN.Gateway)}\n",
    "\n",
    "    # Check if all gateway IDs from A and B are in the name mappings\n",
    "    missing_gateways_a = original_gateway_ids_a - set(name_mappings.keys())\n",
    "    missing_gateways_b = original_gateway_ids_b - set(name_mappings.keys())\n",
    "\n",
    "    if missing_gateways_a:\n",
    "        print(f\"Missing Gateways from Expected in name mappings: {missing_gateways_a}\")\n",
    "    else:\n",
    "        print(\"All gateways from Expected are accounted for in name mappings.\")\n",
    "\n",
    "    if missing_gateways_b:\n",
    "        print(f\"Missing Gateways from Faulty in name mappings: {missing_gateways_b}\")\n",
    "    else:\n",
    "        print(\"All gateways from Faulty are accounted for in name mappings.\") \n",
    "                        \n",
    "def handle_gateways_for_pairs(model_a, model_b): \n",
    "    for mode in ProcessingMode:\n",
    "        for service in model_a[mode.name]:\n",
    "            for subprocess in model_a[mode.name][service]:\n",
    "                gateway_counter = {\"A\": {}, \"B\": {}, \"Shared\": 0}\n",
    "                name_mappings = {}\n",
    "                bpmn_model_a = model_a[mode.name][service][subprocess]['bpmn_graph']\n",
    "                bpmn_model_b = model_b[mode.name][service][subprocess]['bpmn_graph']\n",
    "                gateway_paths_a = extract_gateways(bpmn_model_a)\n",
    "                gateway_paths_b = extract_gateways(bpmn_model_b)\n",
    "                handle_gateways_for_model(gateway_paths_a, gateway_paths_b, gateway_counter, name_mappings, \"A\")\n",
    "                handle_gateways_for_model(gateway_paths_b, gateway_paths_a, gateway_counter, name_mappings, \"B\")\n",
    "                verify_gateway_mappings(bpmn_model_a, bpmn_model_b, name_mappings)\n",
    "                model_a[mode.name][service][subprocess]['bpmn_graph'] = rebuild_bpmn_graph(bpmn_model_a, name_mappings)\n",
    "                model_b[mode.name][service][subprocess]['bpmn_graph'] = rebuild_bpmn_graph(bpmn_model_b, name_mappings)\n",
    "\n",
    "        \n",
    "all_models = copy.deepcopy(all_process_graphs)      \n",
    "        \n",
    "handle_gateways_for_pairs(all_models['Expected'], all_models['Faulty'])\n",
    "# validate_hidden_transitions(all_models['Model A'], all_models['Faulty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e08d40-b1af-43c1-82ef-6a96013f2f30",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b06a2abe-2e00-440b-af66-e12c4618afce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:07.968193Z",
     "start_time": "2023-11-08T23:50:07.955523Z"
    }
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def heuristic(graph: nx.DiGraph, source: str, target: str) -> float:\n",
    "    return 0 # We dont have a grid\n",
    "\n",
    "def a_star_search(graph: nx.DiGraph, start: str, goal: str, logs: list, require_loop=False):\n",
    "    \"\"\"\n",
    "    Modified A* Search Algorithm to find loops.\n",
    "    \n",
    "    :param graph: NetworkX directed graph\n",
    "    :param start: Start node ID\n",
    "    :param goal: Target node ID\n",
    "    :param logs: List of logs with logging_statement_id\n",
    "    :return: Shortest path from start to goal, including loops if required\n",
    "    \"\"\"\n",
    "    log_names = [log['logging_statement_id'] for log in logs]  # Extract log names from logs\n",
    "    open_list = [(0, start, [])]  # Initialize open list with start node\n",
    "    g_score = {start: 0}  # Initialize g_score for start node as 0\n",
    "    \n",
    "    while open_list:  # Loop until open list is empty\n",
    "        f_score, current, path = heapq.heappop(open_list)  # Pop node with lowest f_score\n",
    "        \n",
    "        # If current node is the goal, and loop requirement is met, return path\n",
    "        if current == goal:\n",
    "            if not require_loop or (require_loop and len(path) > 1):\n",
    "                return path + [current]\n",
    "        \n",
    "        for neighbor, edge_data in graph[current].items():  # Loop through neighbors\n",
    "            # Skip nodes in logs except the goal\n",
    "            if neighbor in log_names and neighbor != goal:\n",
    "                continue\n",
    "            \n",
    "            # Calculate tentative g_score for neighbor\n",
    "            tentative_g_score = g_score[current] + edge_data.get('weight', 1)\n",
    "            \n",
    "            # Update g_score if new path is better or neighbor is not in open list\n",
    "            if tentative_g_score < g_score.get(neighbor, float('inf')) or neighbor not in [i[1] for i in open_list]:\n",
    "                g_score[neighbor] = tentative_g_score\n",
    "                f_score = tentative_g_score + heuristic(graph, neighbor, goal)\n",
    "                heapq.heappush(open_list, (f_score, neighbor, path + [current]))\n",
    "                \n",
    "    return None  # Return None if path is not found\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "017090d8-d6ef-46bf-b00c-44098eb5cc3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:08.011181Z",
     "start_time": "2023-11-08T23:50:07.984015Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def initialize_graph_attributes(graph: nx.DiGraph, model: str):\n",
    "    for node in graph.nodes():\n",
    "        if 'count' not in graph.nodes[node]:\n",
    "            graph.nodes[node]['count'] = {'A': 0, 'B': 0}\n",
    "        if model in ['A', 'B'] and model not in graph.nodes[node]['count']:\n",
    "            graph.nodes[node]['count'][model] = 0\n",
    "\n",
    "    for u, v in graph.edges():\n",
    "        if 'count' not in graph[u][v]:\n",
    "            graph[u][v]['count'] = {'A': 0, 'B': 0}\n",
    "        if model in ['A', 'B'] and model not in graph[u][v]['count']:\n",
    "            graph[u][v]['count'][model] = 0\n",
    "\n",
    "def update_path_attributes(graph, path, model):\n",
    "    \"\"\"\n",
    "    Update attributes of edges in the given path.\n",
    "    \"\"\"\n",
    "    for i in range(len(path) - 1):\n",
    "        edge = (path[i], path[i + 1])\n",
    "\n",
    "        if 'count' not in graph.edges[edge]:\n",
    "            graph.edges[edge]['count'] = {'A': 0, 'B': 0}\n",
    "        if model in ['A', 'B']:\n",
    "            graph.edges[edge]['count'][model] += 1\n",
    "\n",
    "    for node in path[1:-1]:  # Exclude the start and end nodes\n",
    "        if 'count' not in graph.nodes[node]:\n",
    "            graph.nodes[node]['count'] = {'A': 0, 'B': 0}\n",
    "        if model in ['A', 'B']:\n",
    "            graph.nodes[node]['count'][model] += 1\n",
    "\n",
    "\n",
    "            \n",
    "def update_graph_from_logs(graph: nx.DiGraph, logs: list, model: str):\n",
    "    \"\"\"\n",
    "    Updates graph attributes based on logs for a specific model.\n",
    "    \"\"\"\n",
    "    initialize_graph_attributes(graph, model)\n",
    "    \n",
    "    # Step 1: Count occurrences of each logging_statement_id in logs\n",
    "    log_count = defaultdict(int)\n",
    "    for log in logs:\n",
    "        log_count[log['logging_statement_id']] += 1\n",
    "        \n",
    "    for i in range(len(logs) - 1):\n",
    "        start_node = logs[i]['logging_statement_id']\n",
    "        end_node = logs[i + 1]['logging_statement_id']\n",
    "        \n",
    "        require_loop = start_node == end_node \n",
    "        \n",
    "        if start_node not in graph or end_node not in graph:\n",
    "            missing_type = []\n",
    "            if start_node not in graph:\n",
    "                missing_type.append(f\"Start node: {start_node}\")\n",
    "            if end_node not in graph:\n",
    "                missing_type.append(f\"End node: {end_node}\")\n",
    "            \n",
    "            # display(HTML(f\"<b><span style='color: orange;'>&emsp;&emsp; --> Missing logging_statement_ids in Model {model}: {', '.join(missing_type)}</span></b>\"))\n",
    "            continue\n",
    "        \n",
    "        # A* search to find path between start_node and end_node\n",
    "        path = a_star_search(graph, start_node, end_node, logs, require_loop)\n",
    "        if path:\n",
    "            update_path_attributes(graph, path, model)                   \n",
    "        # else:\n",
    "        #     # Print a warning if the path is not found\n",
    "        #     display(HTML(f\"<b><span style='color: orange;'>&emsp;&emsp; --> Path not found between {start_node} and {end_node} in Model {model} - Require Loop {require_loop}.</span></b>\"))\n",
    "            \n",
    "            # display(HTML(f\"<b><span style='color: orange;'>&emsp;&emsp; Attempting to add bridge...</span></b>\"))\n",
    "             # Add a dashed edge with isBridge=True attribute\n",
    "#             graph.add_edge(start_node, end_node, style='dashed', isBridge=True)\n",
    "            # plot_graph_with_missing_path(graph, start_node, end_node, title=f\"Missing path {start_node} to {end_node} in Model {model}\")\n",
    "            \n",
    "#             new_path = a_star_search(graph, start_node, end_node)\n",
    "#             if new_path:\n",
    "#                 # Update the path attributes using the new path\n",
    "#                 update_path_attributes(graph, new_path, model)\n",
    "#             else:\n",
    "#                 display(HTML(f\"<b><span style='color: red;'>&emsp;&emsp; --> Path not found between {start_node} and {end_node} in Model {model}</span></b>\"))\n",
    "\n",
    "    # Set each node's count that corresponds to a log to that log count\n",
    "    for node, count in log_count.items():\n",
    "        if node in graph and model in ['A', 'B']:\n",
    "            graph.nodes[node]['count'][model] = count\n",
    "\n",
    "    # # Step 2: Validate counts\n",
    "    # for node, attr in graph.nodes(data=True):\n",
    "    #     if attr.get('type') == 'transition':\n",
    "    #         if f'count_{model}' in attr:\n",
    "    #             if attr[f'count_{model}'] != log_count.get(node, 0):\n",
    "    #                 display(HTML(f\"<b><span style='color: red;'>&emsp;&emsp; --> Mismatch in counts for node {node} in Model {model}. Count in graph: {attr[f'count_{model}']}, Count in logs: {log_count.get(node, 0)}</span></b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7ea47c7-5855-44e8-9968-d94f93adf565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:08.372619Z",
     "start_time": "2023-11-08T23:50:07.998590Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def plot_graph_with_missing_path(graph, start_node, end_node, title=\"Graph with Missing Path\"):\n",
    "    \"\"\"\n",
    "    Visualize the graph highlighting the start and end nodes where path is missing.\n",
    "\n",
    "    :param graph: NetworkX graph\n",
    "    :param start_node: Start node ID\n",
    "    :param end_node: End node ID\n",
    "    :param title: Title for the plot\n",
    "    \"\"\"\n",
    "    A = to_agraph(graph)\n",
    "    \n",
    "    for node in A.nodes():\n",
    "        node_name = node.name\n",
    "        node_type = graph.nodes[node_name].get('type', None)\n",
    "        \n",
    "        if node_type == 'place':\n",
    "            node.attr['shape'] = 'circle'\n",
    "            node.attr['label'] = ''\n",
    "            node.attr['width'] = 0.6\n",
    "            node.attr['height'] = 0.6\n",
    "            \n",
    "            if node_name.startswith(\"sink0\"):  \n",
    "                node.attr['fillcolor'] = 'orange'\n",
    "                node.attr['style'] = 'filled'\n",
    "            elif node_name.startswith(\"source0\"):\n",
    "                node.attr['fillcolor'] = 'green'\n",
    "                node.attr['style'] = 'filled'\n",
    "                \n",
    "        elif node_type == 'transition':\n",
    "            node.attr['shape'] = 'rectangle'\n",
    "            \n",
    "            if node_name.startswith(\"hid_\"):\n",
    "                node.attr['style'] = 'filled'\n",
    "                node.attr['fillcolor'] = 'black'\n",
    "                node.attr['fontcolor'] = 'black'  # Make label invisible\n",
    "                node.attr['penwidth'] = 5\n",
    "                node.attr['fontcolor'] = 'white'        \n",
    "        \n",
    "        if (start_node == end_node):\n",
    "            if node_name == start_node:\n",
    "                node.attr['color'] = 'purple'  # Highlight start_node\n",
    "                node.attr['penwidth'] = 7  # Make stroke thicker\n",
    "        else:\n",
    "            if node_name == start_node:\n",
    "                node.attr['color'] = 'red'  # Highlight start_node\n",
    "                node.attr['penwidth'] = 7  # Make stroke thicker\n",
    "\n",
    "            elif node_name == end_node:\n",
    "                node.attr['color'] = 'blue'  # Highlight end_node\n",
    "                node.attr['penwidth'] = 7  # Make stroke thicker\n",
    "\n",
    "            \n",
    "    for edge in A.edges():\n",
    "        edge_attr = graph.get_edge_data(edge[0], edge[1])\n",
    "        if edge_attr.get('isBridge'):\n",
    "            edge.attr['style'] = 'dashed'\n",
    "            edge.attr['color'] = 'purple'\n",
    "\n",
    "    A.layout(prog='dot')\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    A.draw(f\"{title}.png\")\n",
    "    img = plt.imread(f\"{title}.png\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1708d-3bc7-491b-9671-de9ff66a1505",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff4e8d81-d782-421d-b453-c86edb6b9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_agraph import to_agraph\n",
    "\n",
    "def plot_graph(graph, G1, G2, title):\n",
    "    \"\"\"\n",
    "    Visualize the graph highlighting the start and end nodes where path is missing.\n",
    "\n",
    "    :param graph: NetworkX graph\n",
    "    :param start_node: Start node ID\n",
    "    :param end_node: End node ID\n",
    "    :param title: Title for the plot\n",
    "    \"\"\"\n",
    "    # Identify unique nodes and edges in both graphs\n",
    "    unique_nodes_G1 = set(G1.nodes()) - set(G2.nodes())\n",
    "    unique_nodes_G2 = set(G2.nodes()) - set(G1.nodes())\n",
    "    \n",
    "    unique_edges_G1 = set(G1.edges()) - set(G2.edges())\n",
    "    unique_edges_G2 = set(G2.edges()) - set(G1.edges())\n",
    "    \n",
    "    A = to_agraph(graph)\n",
    "    \n",
    "    for node in A.nodes():\n",
    "        node_name = node.name\n",
    "        node_type = graph.nodes[node_name].get('type', None)\n",
    "        \n",
    "        if node_type == 'StartEvent':\n",
    "            node.attr['shape'] = 'circle'\n",
    "            node.attr['label'] = ''\n",
    "            node.attr['width'] = 0.6\n",
    "            node.attr['height'] = 0.6\n",
    "            node.attr['fillcolor'] = 'green'\n",
    "            node.attr['style'] = 'filled'    \n",
    "            \n",
    "        elif node_type == 'NormalEndEvent':\n",
    "            node.attr['shape'] = 'circle'\n",
    "            node.attr['label'] = ''\n",
    "            node.attr['width'] = 0.6\n",
    "            node.attr['height'] = 0.6\n",
    "            node.attr['fillcolor'] = 'orange'\n",
    "            node.attr['style'] = 'filled'      \n",
    "            \n",
    "        elif node_type == 'Task':\n",
    "            node.attr['shape'] = 'rectangle'\n",
    "          \n",
    "        elif node_type == 'ExclusiveGateway':\n",
    "            node.attr['shape'] = 'diamond'\n",
    "            node.attr['label'] = 'X'\n",
    "            node.attr['width'] = 0.6\n",
    "            node.attr['height'] = 0.6\n",
    "            node.attr['style'] = 'filled'\n",
    "\n",
    "        elif node_type == 'ParallelGateway':\n",
    "            node.attr['shape'] = 'diamond'\n",
    "            node.attr['label'] = '+'\n",
    "            node.attr['width'] = 0.6\n",
    "            node.attr['height'] = 0.6\n",
    "            node.attr['style'] = 'filled'  \n",
    "        \n",
    "        else :\n",
    "            print(f\"Could not handle node of type: {node_type}\")\n",
    "         \n",
    "             \n",
    "        if node_name in unique_nodes_G1:  # Unique to G1\n",
    "            node.attr['color'] = 'lightblue'\n",
    "            node.attr['style'] = 'filled'\n",
    "        elif node_name in unique_nodes_G2:  # Unique to G2\n",
    "            node.attr['color'] = 'red'\n",
    "            node.attr['style'] = 'filled'\n",
    "            \n",
    "    for edge in A.edges():\n",
    "        edge_attr = graph.get_edge_data(edge[0], edge[1])\n",
    "        if edge_attr.get('isBridge'):\n",
    "            edge.attr['style'] = 'dashed'\n",
    "            edge.attr['color'] = 'purple'\n",
    "            \n",
    "        if (edge[0], edge[1]) in unique_edges_G1:  # Unique to G1\n",
    "            edge.attr['color'] = 'lightblue'\n",
    "            edge.attr['penwidth'] = 5\n",
    "        elif (edge[0], edge[1]) in unique_edges_G2:  # Unique to G2\n",
    "            edge.attr['color'] = 'red'\n",
    "            edge.attr['penwidth'] = 5\n",
    "            \n",
    "            \n",
    "    A.layout(prog='dot')\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    A.draw(f\"Graphs/{title}.png\")\n",
    "    img = plt.imread(f\"Graphs/{title}.png\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a1e6714-d9b3-4b67-85c8-af2e89f7b9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:08.709503Z",
     "start_time": "2023-11-08T23:50:08.380118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../Data/Reproduced Experiment/Diagnosis/Models/Part B/model_partB.json\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from networkx.drawing.nx_agraph import to_agraph, write_dot\n",
    "from pm4py.objects.petri_net.obj import PetriNet\n",
    "from pm4py.objects.petri_net.utils.networkx_graph import create_networkx_directed_graph\n",
    "from pm4py.objects.petri_net.utils import petri_utils\n",
    "\n",
    "def bpmn_to_networkx(bpmn_model):\n",
    "    \"\"\"\n",
    "    Convert a BPMN model to a NetworkX graph.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes to the NetworkX graph using names\n",
    "    for node in bpmn_model.get_nodes():\n",
    "        G.add_node(node.name, label=node.name, type=type(node).__name__)\n",
    "\n",
    "    # Add edges to the NetworkX graph using names\n",
    "    for flow in bpmn_model.get_flows():\n",
    "        source_name = flow.source.name if hasattr(flow.source, 'name') else None\n",
    "        target_name = flow.target.name if hasattr(flow.target, 'name') else None\n",
    "        if source_name and target_name:\n",
    "            G.add_edge(source_name, target_name)\n",
    "\n",
    "    return G\n",
    "\n",
    "def annotate_union_graph(G1, G2, G_union):\n",
    "    \"\"\"\n",
    "    Annotate the union graph nodes and edges based on their presence in G1 and G2.\n",
    "    \"\"\"\n",
    "    for node in G_union.nodes():\n",
    "        G_union.nodes[node]['in_graph'] = {\n",
    "            'A': node in G1.nodes(),\n",
    "            'B': node in G2.nodes()\n",
    "        }\n",
    "        # G_union.nodes[node]['count'] = {\n",
    "        #     'A': G1.nodes[node]['count'] if node in G1.nodes() else 0,\n",
    "        #     'B': G2.nodes[node]['count'] if node in G2.nodes() else 0\n",
    "        # }\n",
    "        \n",
    "    for u, v, attributes in G_union.edges(data=True):\n",
    "        edge = (u, v)\n",
    "        G_union.edges[edge]['in_graph'] = {\n",
    "            'A': edge in G1.edges(),\n",
    "            'B': edge in G2.edges()\n",
    "        }\n",
    "        # G_union.edges[edge]['count'] = {\n",
    "        #     'A': G1.edges[edge]['count'] if edge in G1.edges() else 0,\n",
    "        #     'B': G2.edges[edge]['count'] if edge in G2.edges() else 0\n",
    "        # }\n",
    "        \n",
    "def save_and_plot_all_models(all_models, raw_log_data_A, raw_log_data_B):\n",
    "    \"\"\"\n",
    "    Saves and plots all models based on raw log data.\n",
    "    \"\"\"\n",
    "    json_data = {}\n",
    "    \n",
    "    # Extract models A and B\n",
    "    model_a_data = all_models.get('Expected', {})\n",
    "    model_b_data = all_models.get('Faulty', {})\n",
    "\n",
    "    service_names = []\n",
    "    \n",
    "    for mode_name, services in model_a_data.items():\n",
    "        json_data[mode_name] = {\n",
    "        \"viewType\": \"Contrast\", \n",
    "        \"graphType\": \"BPMN\", \n",
    "        \"data\": {} \n",
    "        }\n",
    "        \n",
    "        for service_name, subprocesses in services.items():\n",
    "            service_names.append(service_name)\n",
    "            \n",
    "            json_data[mode_name][\"data\"][service_name] = {} \n",
    "            \n",
    "            for subprocess_name, model_data in subprocesses.items():\n",
    "                # Extract and initialize Petri Nets for Expected and Faulty\n",
    "                bpmn_model_A = model_data.get('bpmn_graph', None)\n",
    "                bpmn_model_B = model_b_data.get(mode_name, {}).get(service_name, {}).get(subprocess_name, {}).get('bpmn_graph', None)\n",
    "                if bpmn_model_A is None or bpmn_model_B is None:\n",
    "                    continue\n",
    "                \n",
    "                # print(bpmn_model_B)\n",
    "                # print(f\"Structure of BPMN Faulty - {subprocess_name}:\")\n",
    "                # for node in bpmn_model_B.get_nodes():\n",
    "                #     print(f\"Node: {node.get_name()}, Type: {type(node).__name__}\")\n",
    "                # for flow in bpmn_model_B.get_flows():\n",
    "                #     print(f\"Flow: {flow.get_source().get_name()} -> {flow.get_target().get_name()}\")\n",
    "                    \n",
    "                G_A  = bpmn_to_networkx(bpmn_model_A)\n",
    "                G_B  = bpmn_to_networkx(bpmn_model_B)\n",
    "                \n",
    "                # Fetch corresponding logs\n",
    "                log_data_A = raw_log_data_A.get(mode_name, {}).get(service_name, {}).get(subprocess_name, {})\n",
    "                log_data_B = raw_log_data_B.get(mode_name, {}).get(service_name, {}).get(subprocess_name, {})\n",
    "                \n",
    "                # Update graphs based on logs\n",
    "                update_graph_from_logs(G_A, log_data_A.get('logs', []), \"A\")\n",
    "                update_graph_from_logs(G_B, log_data_B.get('logs', []), \"B\")\n",
    "                \n",
    "                # Generate the title and file path\n",
    "                title = f\"{service_name} - {subprocess_name}\" \n",
    "                # file_path_comparison = f\"Graphs/Comparison/{mode_name}/{title}.dot\"\n",
    "\n",
    "                # Create union graph and annotate\n",
    "                G_union = nx.compose(G_A, G_B)\n",
    "                annotate_union_graph(G_A, G_B, G_union)\n",
    "                \n",
    "                # Convert node attributes to strings\n",
    "                for node, attr in G_union.nodes(data=True):\n",
    "                    updated_attr = {k: str(v) for k, v in attr.items()}\n",
    "                    G_union.nodes[node].update(updated_attr)\n",
    "\n",
    "                # Convert edge attributes to strings\n",
    "                for u, v, attr in G_union.edges(data=True):\n",
    "                    updated_attr = {k: str(v) for k, v in attr.items()}\n",
    "                    G_union[u][v].update(updated_attr)\n",
    "                                \n",
    "                # plot_graph(G_union, G_A, G_B, title)\n",
    "                \n",
    "                # Save the graph in .dot format and read its content\n",
    "                temp_dot_file = f\"temp_union_graph.dot\"\n",
    "                write_dot(G_union, temp_dot_file)\n",
    "                with open(temp_dot_file, 'r') as file:\n",
    "                    graph_content = file.read()\n",
    "                os.remove(temp_dot_file)  # Remove the temporary file\n",
    "                \n",
    "                json_data[mode_name][\"data\"][service_name][subprocess_name] = {\n",
    "                    \"graphData\": graph_content,\n",
    "                    \"logData\": {\n",
    "                    \"A\":log_data_A,\n",
    "                    \"B\": log_data_B,    \n",
    "                    },\n",
    "                }\n",
    "         \n",
    "         # Add Blackbox Services\n",
    "        service_names.append(\"web-app\")\n",
    "        service_names.append(\"mail-service\")\n",
    "\n",
    "        json_data[mode_name][\"services\"] = service_names\n",
    "        \n",
    "        save_json(json_data[mode_name], f\"../Data/Reproduced Experiment/Diagnosis/Models/Part B/model_partB.json\")\n",
    "        \n",
    "def save_json(data, file_path):\n",
    "    \"\"\"Save a Python dictionary to a JSON file.\"\"\"\n",
    "    print(f\"Saving {file_path}\")\n",
    "    \n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        \n",
    "        \n",
    "# Call the function to save and plot all models\n",
    "save_and_plot_all_models(all_models, logs_A, logs_B)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51f554-43d2-4abc-b3be-5712ee115b1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:08.719209Z",
     "start_time": "2023-11-08T23:50:08.710978Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e69502-08be-4798-bbbc-d51221710e75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
