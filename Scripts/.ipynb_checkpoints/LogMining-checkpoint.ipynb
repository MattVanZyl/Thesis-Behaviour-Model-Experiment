{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd932ac-3f58-4cfe-ad55-86e45488ec7c",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ebac8-203b-4d09-9a99-eb86c9101f4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:24:32.848785Z",
     "start_time": "2023-11-10T01:24:26.820793Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas==2.0.3\n",
    "%pip install tqdm==4.66.1\n",
    "%pip install pm4py==2.7.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c1127c-259f-4071-b755-adce2ba65d43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:24:35.091309Z",
     "start_time": "2023-11-10T01:24:32.844409Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import concurrent.futures\n",
    "from collections import defaultdict, Counter\n",
    "# Process Mining\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "from io import StringIO\n",
    "import os\n",
    "import json\n",
    "from uuid import uuid4\n",
    "\n",
    "from collections import Counter\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import spacy\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from enum import Enum, auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ac388-de77-44c1-9e79-2ab5e0a7196d",
   "metadata": {},
   "source": [
    "# Log Mining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04fbd3bd-7725-474a-b96c-5de43727eb65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:24:35.116857Z",
     "start_time": "2023-11-10T01:24:35.096591Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "def _extract_log_data(logs, regex_pattern, service_name):\n",
    "    log_data = []\n",
    "    pattern = re.compile(regex_pattern)\n",
    "    lines = logs.split('\\n')\n",
    "\n",
    "    for line in tqdm(lines, total=len(lines), desc=f\"Processing log data for {service_name}\"):\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            log_message = match.groupdict()\n",
    "            log_message['service'] = service_name\n",
    "            log_message['message'] = match.groupdict()['message'].rstrip() # Remove whitespace at message end\n",
    "            log_message['id'] = str(uuid4())\n",
    "            log_data.append(log_message)          \n",
    "        else:       \n",
    "            if log_data: # If the line doesn't match the pattern, append it to the previous log message\n",
    "                log_data[-1]['message'] += line.strip()\n",
    "\n",
    "    return log_data\n",
    "\n",
    "# ======================================================================\n",
    "def _extract_logging_statements(log_data):  \n",
    "    # Group the logs\n",
    "    log_groups = defaultdict(list)\n",
    "    for log in log_data:\n",
    "        key = (log[\"service\"], log[\"process\"], log[\"subprocess\"], log[\"level\"], log[\"class\"], log[\"method\"], log[\"file\"], log[\"line\"])\n",
    "        log_groups[key].append(log)\n",
    "    \n",
    "    # Assign a logging statement ID to each unique combination and count their occurrences\n",
    "    logging_statements = []\n",
    "    logging_statement_id_mapping = {}\n",
    "    \n",
    "    for key, logs in tqdm(log_groups.items(), desc=\"Assigning logging statement IDs\"):\n",
    "        service, process, subprocess, level, class_name, method, file, line = key\n",
    "        associated_log_ids = [log['id'] for log in logs]  # List of log IDs\n",
    "           \n",
    "        logging_statement_id = f\"{service} - {file} - {method} - {line}\"\n",
    "        \n",
    "        logging_statement = {\n",
    "            \"logging_statement_id\": logging_statement_id,\n",
    "            \"service\": service,\n",
    "            \"process\": process,\n",
    "            \"subprocess\" : subprocess,\n",
    "            \"level\" : level,\n",
    "            \"class\": class_name,\n",
    "            \"method\": method,\n",
    "            \"file\": file,\n",
    "            \"line\": line,\n",
    "            \"associated_logs\": len(logs),\n",
    "            \"associated_log_ids\": associated_log_ids           \n",
    "        }\n",
    "        \n",
    "        logging_statements.append(logging_statement)\n",
    "        logging_statement_id_mapping[key] = logging_statement_id\n",
    "\n",
    "    # Assign the logging statement ID to each log\n",
    "    for log in tqdm(log_data, desc=\"Assigning logging statement IDs to logs\"):\n",
    "        key = (log[\"service\"], log[\"process\"], log[\"subprocess\"], log[\"level\"], log[\"class\"], log[\"method\"], log[\"file\"], log[\"line\"])\n",
    "        log[\"logging_statement_id\"] = logging_statement_id_mapping[key]\n",
    "\n",
    "    return log_data, logging_statements\n",
    "    \n",
    "def process_logs_variant(all_logs, logging_statements, regex_pattern):\n",
    "    processed_logs = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "    # Create a dictionary for BPMN-specific logs\n",
    "    bpmn_logs = defaultdict(list)\n",
    "\n",
    "    for log in all_logs:\n",
    "        service, subprocess = log.get('service'), log.get('subprocess')\n",
    "\n",
    "        bpmn_key = (service, subprocess)\n",
    "\n",
    "        bpmn_logs[bpmn_key].append(log.copy())  # Store a copy of the log\n",
    "\n",
    "    for (bpmn_service, bpmn_subprocess), bpmn_specific_logs in bpmn_logs.items():\n",
    "\n",
    "        target_logs = [log for log in all_logs if log.get('service') == bpmn_service and log.get('subprocess') == bpmn_subprocess]\n",
    "        target_logging_statements = [statement for statement in logging_statements if statement.get('service') == bpmn_service and statement.get('subprocess') == bpmn_subprocess]\n",
    "\n",
    "        processed_logs[bpmn_service][bpmn_subprocess] = {\n",
    "            \"logs\": target_logs,\n",
    "            \"log_statements\": target_logging_statements,\n",
    "        }\n",
    "\n",
    "    return processed_logs   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d654c-8c0e-4829-bb2a-aabb9e2260c5",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e96d9310-7734-4623-ba2b-b38c2b42938f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:24:47.173388Z",
     "start_time": "2023-11-10T01:24:35.115634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing logs for ../Data/Reproduced Experiment/Raw/Logs/Part A ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2056e3f3a7cd42e1bf8d9a07e0994077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing log data for experiment-service:   0%|          | 0/60200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1eb6773c1314b6fab8f289e552eddeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing log data for pcm-service:   0%|          | 0/10675 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bc6ebfa6104974a90e021ccbed96f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assigning logging statement IDs:   0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b341faaf44d744c6a7c397a6444aed69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assigning logging statement IDs to logs:   0%|          | 0/4046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing logs for Expected ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e121e4f3ad4e53aad8ed429673cb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing log data for experiment-service:   0%|          | 0/218146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f976c5889cb4093bce928a320eebe11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing log data for pcm-service:   0%|          | 0/7839 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e6d51e2e8742adbd397435e1393a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assigning logging statement IDs:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc582ad9cd254561878fe11f56912701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assigning logging statement IDs to logs:   0%|          | 0/1013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing logs for Faulty ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3662393aae574f34905e83033b63ace1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing log data for experiment-service:   0%|          | 0/218846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ce5ea1d35e40409e37ad0f058b1794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing log data for pcm-service:   0%|          | 0/6841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed log: {'timestamp': '2023-11-10 12:22:08.460000', 'level': 'WARN', 'pid': '92240', 'process': 'Email Researcher', 'subprocess': 'Handle Notification Request', 'thread': 'MessageBroker-5', 'class': 'NotificationService', 'method': 'lambda$serviceNotification$3', 'file': 'NotificationService.java', 'line': '187', 'user': 'unknown', 'message': '[FAULT] Fault site - RecipientParticipants is null and is not assigned a value in the RESEARCHER message target type.', 'service': 'experiment-service', 'id': '6c94da65-1b31-4760-9e55-6029678f1b63'}\n",
      "Removed log: {'timestamp': '2023-11-10 12:22:08.460000', 'level': 'WARN', 'pid': '92240', 'process': 'Email Researcher', 'subprocess': 'Handle Notification Request', 'thread': 'MessageBroker-5', 'class': 'NotificationService', 'method': 'lambda$serviceNotification$3', 'file': 'NotificationService.java', 'line': '239', 'user': 'unknown', 'message': '[FAULT] Because RecipientParticipants is null, it does not send an email for the message target type RESEARCHER', 'service': 'experiment-service', 'id': '41e4b77a-536f-42be-bb5b-35f7f9828d53'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bb1c52c43d46c0b09a6ee0e1a8fa21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assigning logging statement IDs:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bf4ead8e3344eeae173fac6fdbae2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assigning logging statement IDs to logs:   0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done\n",
      "\n",
      "Loading data from: ../Data/Reproduced Experiment/Preprocessed/Logs/Part B/Expected/processed_logs.json\n",
      "Loaded data for Expected/processed_logs.json.\n",
      "Loading data from: ../Data/Reproduced Experiment/Preprocessed/Logs/Part B/Faulty/processed_logs.json\n",
      "Loaded data for Faulty/processed_logs.json.\n",
      "Collecting all unique services across models.\n",
      "Unique services found: 2 services.\n",
      "Ensuring all services are present for model: Expected/processed_logs.json\n",
      "Ensuring all services are present for model: Faulty/processed_logs.json\n",
      "Writing updated data to: ../Data/Reproduced Experiment/Preprocessed/Logs/Part B/Expected/processed_logs.json\n",
      "Finished writing data for Expected/processed_logs.json.\n",
      "Writing updated data to: ../Data/Reproduced Experiment/Preprocessed/Logs/Part B/Faulty/processed_logs.json\n",
      "Finished writing data for Faulty/processed_logs.json.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()\n",
    "    \n",
    "regex_pattern = (  \n",
    "    r'(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{6}) '\n",
    "    r'(?P<level>[A-Z]+) '\n",
    "    r'(?P<pid>\\d+) '\n",
    "    r'\\s*---\\s* '\n",
    "    r'(?:\\[(?P<process>[^\\]]+)\\]) '\n",
    "    r'(?:\\[(?P<subprocess>[^\\]]+)\\]) '\n",
    "    r'\\s*---\\s* '\n",
    "    r'\\[(?P<thread>[^\\]]+)\\] '\n",
    "    r'(?P<class>[^\\.]+)\\.(?P<method>[^\\(]+)\\((?P<file>.*\\.java):(?P<line>\\d+)\\) '\n",
    "    r'\\s*-\\s* '\n",
    "    r'(?P<user>.*) '\n",
    "    r'\\s*:\\s* '\n",
    "    r'(?P<message>.*)'\n",
    ")\n",
    "\n",
    "def process_multiple_models(relative_path, process_name, use_contrast_mode=False):\n",
    "    if use_contrast_mode:\n",
    "        model_dirs = [\"Expected\", \"Faulty\"]\n",
    "    else:\n",
    "        model_dirs = [relative_path]\n",
    "\n",
    "    for model in model_dirs:\n",
    "        print(f\"=== Processing logs for {model} ===\")\n",
    "        log_files = []\n",
    "\n",
    "        if use_contrast_mode:\n",
    "            log_dir = os.path.join(relative_path, model)\n",
    "        else:\n",
    "            log_dir = relative_path\n",
    "\n",
    "        for file in os.listdir(log_dir):\n",
    "            if file.endswith('.log'):\n",
    "                service_name = file[:-4]  # Extract service name\n",
    "                file_path = os.path.join(log_dir, file)\n",
    "                log_files.append((read_file(file_path), service_name))\n",
    "\n",
    "        all_logs = []\n",
    "        for log_file, service_name in log_files:\n",
    "            log_data = _extract_log_data(log_file, regex_pattern, service_name)\n",
    "            log_data = [log for log in log_data if log.get('process') == process_name]\n",
    "\n",
    "            # Add service name to logs\n",
    "            for log in log_data:\n",
    "                log['service'] = service_name\n",
    "            all_logs.extend(log_data)\n",
    "\n",
    "        removed_logs = []  # Store removed logs\n",
    "\n",
    "        # Identify and print logs to be removed\n",
    "        for log in all_logs:\n",
    "            if '[FAULT]' in log['message']:\n",
    "                removed_logs.append(log)\n",
    "                print(f\"Removed log: {log}\")  # Print each removed log\n",
    "\n",
    "        # Remove logs of level WARN and logs containing the word \"publish\"\n",
    "        all_logs = [log for log in all_logs if log not in removed_logs]\n",
    "\n",
    "        # Extract logging statements once\n",
    "        all_logs, logging_statements = _extract_logging_statements(all_logs)\n",
    "\n",
    "        processed_logs = process_logs_variant(all_logs, logging_statements, regex_pattern)\n",
    "\n",
    "        # Modify output_dir to include both the last part of the relative_path and the model name in contrast mode\n",
    "        if use_contrast_mode:\n",
    "            base_name = os.path.basename(os.path.normpath(relative_path))\n",
    "            output_dir_name = os.path.join(base_name, model)\n",
    "        else:\n",
    "            output_dir_name = os.path.basename(os.path.normpath(relative_path))\n",
    "\n",
    "        output_dir = os.path.join('../Data/Reproduced Experiment/Preprocessed/Logs', output_dir_name)\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        with open(os.path.join(output_dir, f'processed_logs.json'), 'w') as f:\n",
    "            json.dump(processed_logs, f, indent=2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # CONTRAST GRAPH     \n",
    "    # process_multiple_models(\"./Raw Logs/Contrast\", \"Email Researcher\", True)\n",
    "    \n",
    "    # # COMPLEX GRAPH    \n",
    "    # process_multiple_models(\"./Raw Logs/Single\", \"PCM Data Export\", False)\n",
    "    \n",
    "    # COMPLEX GRAPH    \n",
    "    process_multiple_models(\"../Data/Reproduced Experiment/Raw/Logs/Part A\", \"Participant Enrolment\", False)\n",
    "    process_multiple_models(\"../Data/Reproduced Experiment/Raw/Logs/Part B\", \"Email Researcher\", True)                        \n",
    "    \n",
    "    print(\"\\nDone\\n\")\n",
    "    \n",
    "    \n",
    "def fill_missing_keys(base_dir, model_names):\n",
    "    \"\"\"Ensure both models have keys for all services and subprocesses.\"\"\"\n",
    "    all_data = {}\n",
    "\n",
    "    # Load all data first\n",
    "    for model_name in model_names:\n",
    "        model_path = f'{base_dir}/{model_name}'\n",
    "        print(f\"Loading data from: {model_path}\")  # Output progress\n",
    "        with open(model_path, 'r') as f:\n",
    "            all_data[model_name] = json.load(f)\n",
    "        print(f\"Loaded data for {model_name}.\")  # Output confirmation\n",
    "\n",
    "    # Get all unique services and subprocesses\n",
    "    all_services = set()\n",
    "    print(\"Collecting all unique services across models.\")  # Progress output\n",
    "    for model_data in all_data.values():\n",
    "        all_services.update(model_data.keys())\n",
    "    print(f\"Unique services found: {len(all_services)} services.\")  # Output the number of services found\n",
    "\n",
    "    # Now ensure each model has all services\n",
    "    for model_name, model_data in all_data.items():\n",
    "        print(f\"Ensuring all services are present for model: {model_name}\")  # Progress output\n",
    "        for service in all_services:\n",
    "            if service not in model_data:\n",
    "                # Add empty service if not present\n",
    "                print(f\"Adding missing service '{service}' to {model_name}\")  # Progress output\n",
    "                all_data[model_name][service] = {}\n",
    "\n",
    "            # For each service, ensure all subprocesses are present\n",
    "            all_subprocesses = set()\n",
    "            for other_model_data in all_data.values():\n",
    "                all_subprocesses.update(other_model_data.get(service, {}).keys())\n",
    "\n",
    "            for subprocess in all_subprocesses:\n",
    "                if subprocess not in model_data[service]:\n",
    "                    # Add empty subprocess if not present\n",
    "                    print(f\"Adding missing subprocess '{subprocess}' under service '{service}' in {model_name}\")  # Progress output\n",
    "                    all_data[model_name][service][subprocess] = {\"logs\": [], \"log_statements\": []}\n",
    "\n",
    "    # Write the modified data back to the JSON files\n",
    "    for model_name in model_names:\n",
    "        model_path = f'{base_dir}/{model_name}'\n",
    "        print(f\"Writing updated data to: {model_path}\")  # Output progress\n",
    "        with open(model_path, 'w') as f:\n",
    "            json.dump(all_data[model_name], f, indent=2)\n",
    "        print(f\"Finished writing data for {model_name}.\")  # Output confirmation\n",
    "\n",
    "# Define the base directory and model names\n",
    "base_dir = '../Data/Reproduced Experiment/Preprocessed/Logs/Part B'\n",
    "model_names = ['Expected/processed_logs.json', 'Faulty/processed_logs.json']\n",
    "\n",
    "# Call the function\n",
    "fill_missing_keys(base_dir, model_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40299cd7-60b0-43f9-9d2d-9da7a9388a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
