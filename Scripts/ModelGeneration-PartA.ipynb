{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf1550f-0054-4b28-989a-cf9eafd85ec7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98c6b3-b259-4a1a-b73f-5fea16722f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas==2.0.3\n",
    "%pip install tqdm==4.66.1\n",
    "%pip install pm4py==2.7.8.2\n",
    "%pip install pygraphviz==1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8d56b2a06a0dca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:06.407815Z",
     "start_time": "2023-11-08T23:49:59.832275Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from enum import Enum, auto\n",
    "from IPython.display import display, HTML\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "from pm4py.objects.bpmn.obj import BPMN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d8160e-c7ef-4c76-aaa5-ed7d2a66a6b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Process Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0500daa-6b2d-4148-b5ff-6f66e67240f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:07.696172Z",
     "start_time": "2023-11-08T23:50:06.428987Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color: green;'>All logging_statement_ids accounted for</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style='color: green;'>All logging_statement_ids accounted for</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style='color: green;'>All logging_statement_ids accounted for</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style='color: green;'>All logging_statement_ids accounted for</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><span style='color: blue;'>&emsp;&emsp; --> DONE</span></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "from pm4py.objects.bpmn.obj import BPMN\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def print_bpmn(bpmn_graph):\n",
    "    # Print the structure of the BPMN graph\n",
    "    for node in bpmn_graph.get_nodes():\n",
    "        print(f\"Node: {node.get_name()}, Type: {type(node).__name__}\")\n",
    "    for flow in bpmn_graph.get_flows():\n",
    "        print(f\"Flow: {flow.get_source().get_name()} -> {flow.get_target().get_name()}\")\n",
    "\n",
    "class ProcessingMode(Enum):\n",
    "    MODEL = auto()\n",
    "\n",
    "def discover_bpmn_graph(log_data):        \n",
    "    \"\"\"Extract Petri net graph from log data.\"\"\"\n",
    "    logs_df = pd.DataFrame(log_data)   \n",
    "    logs_df['timestamp'] = pd.to_datetime(logs_df['timestamp'])\n",
    "    logs_df['logging_statement_id'] = logs_df['logging_statement_id'].astype(str)\n",
    "    logs_df['case_id'] = '01'\n",
    "    \n",
    "    params = {\n",
    "        'dependency_threshold': 0.45,\n",
    "        'and_threshold': 0.65,\n",
    "        'loop_two_threshold': 0.5,\n",
    "        'activity_key': 'logging_statement_id',\n",
    "        'case_id_key': 'case_id',\n",
    "        'timestamp_key': 'timestamp'\n",
    "    }\n",
    "    net, im, fm = pm4py.discover_petri_net_heuristics(logs_df, **params)\n",
    "    \n",
    "    bpmn_graph = pm4py.convert_to_bpmn(net, im, fm)\n",
    "    \n",
    "    # Get all unique logging_statement_ids from DataFrame\n",
    "    unique_logging_statement_ids = set(logs_df['logging_statement_id'].unique())\n",
    "\n",
    "    # Check if all unique_logging_statement_ids are in the tasks of the BPMN graph\n",
    "    bpmn_tasks = [node for node in bpmn_graph.get_nodes() if isinstance(node, BPMN.Task)]\n",
    "    task_labels = set(task.get_name() for task in bpmn_tasks)\n",
    "\n",
    "    missing_ids = unique_logging_statement_ids - task_labels\n",
    "\n",
    "    if missing_ids:\n",
    "        missing_ids_str = '<br>'.join(missing_ids)\n",
    "        display(HTML(f\"<span style='color: red;'>Missing logging_statement_ids in BPMN graph:<br>{missing_ids_str}</span>\"))\n",
    "    else:\n",
    "        display(HTML(f\"<span style='color: green;'>All logging_statement_ids accounted for</span>\"))\n",
    "            \n",
    "    bpmn_graph = pm4py.convert_to_bpmn(net, im, fm)\n",
    "    \n",
    "    return bpmn_graph\n",
    "\n",
    "def load_logs(base_dir, mode):\n",
    "    \"\"\"Load logs from a file.\"\"\"\n",
    "    with open(f'{base_dir}/processed_logs_mode_{mode.name}.json') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_unique_services_and_subprocesses(logs, logs_B=None):\n",
    "    \"\"\"\n",
    "    Get unique services and subprocesses.\n",
    "    If logs_B is provided, it returns the unique items between the two sets of logs.\n",
    "    If logs_B is None, it returns the unique items from logs.\n",
    "    \"\"\"\n",
    "    if logs_B:\n",
    "        unique_services = set(logs.keys()).union(set(logs_B.keys()))\n",
    "    else:\n",
    "        unique_services = set(logs.keys())\n",
    "\n",
    "    unique_subprocesses = {}\n",
    "    for service in unique_services:\n",
    "        subprocesses_A = set(logs.get(service, {}).keys())\n",
    "        subprocesses_B = set(logs_B.get(service, {}).keys()) if logs_B else set()\n",
    "        unique_subprocesses[service] = subprocesses_A.union(subprocesses_B)\n",
    "\n",
    "    return unique_services, unique_subprocesses\n",
    "\n",
    "def create_empty_bpmn():\n",
    "    \"\"\"Create an empty BPMN graph with a single start event.\"\"\"\n",
    "    empty_bpmn = BPMN(name='Empty BPMN')\n",
    "    return empty_bpmn\n",
    "\n",
    "def process_graphs_and_save(logs, all_process_graphs, unique_services, unique_subprocesses):\n",
    "    \"\"\"Generate process graphs and save them.\"\"\"\n",
    "    for mode in ProcessingMode:\n",
    "        all_process_graphs.setdefault(mode.name, {})\n",
    "        for service in unique_services[mode]:\n",
    "            for subprocess in unique_subprocesses[mode].get(service, {}):\n",
    "                target_logs = logs[mode.name].get(service, {}).get(subprocess, {}).get('logs', [])\n",
    "                \n",
    "                 # Check if target_logs is empty\n",
    "                if not target_logs:\n",
    "                    print(f\"No logs found for {service} -> {subprocess} in {mode.name}. Creating an empty BPMN graph.\")\n",
    "                    bpmn_graph = create_empty_bpmn()\n",
    "                else:\n",
    "                    bpmn_graph = discover_bpmn_graph(target_logs)\n",
    "                    \n",
    "                # print_bpmn(bpmn_graph)    \n",
    "                \n",
    "                # Save net along with its initial and final markings\n",
    "                all_process_graphs[mode.name].setdefault(service, {})[subprocess] = {\n",
    "                    'bpmn_graph': bpmn_graph,\n",
    "                }\n",
    "\n",
    "def process_logs(logs, process_graph):\n",
    "    unique_services = {}\n",
    "    unique_subprocesses = {}\n",
    "    for mode in ProcessingMode:\n",
    "        unique_services[mode], unique_subprocesses[mode] = get_unique_services_and_subprocesses(logs[mode.name])\n",
    "\n",
    "    process_graphs_and_save(logs, process_graph, unique_services, unique_subprocesses)\n",
    "    display(HTML(f\"<b><span style='color: blue;'>&emsp;&emsp; --> DONE</span></b>\"))\n",
    "\n",
    "\n",
    "process_graph = {}\n",
    "logs = {mode.name: load_logs('../Data/Reproduced Experiment/Preprocessed/Logs/Part A/', mode) for mode in ProcessingMode}    \n",
    "process_logs(logs, process_graph)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f386f5b-9c33-4abd-93e3-57058f2079b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gateway Identification Using Path-Based Signiture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38fdf07b-6c36-4dda-8378-ba72e640ea64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:07.955035Z",
     "start_time": "2023-11-08T23:50:07.695595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def extract_gateways(bpmn_graph):\n",
    "    \"\"\"\n",
    "    This function extracts gateways from a BPMN model and identifies them based on their path-based signatures.\n",
    "    It maps each gateway to the unique paths leading from it, considering only Events and Tasks as path terminators.\n",
    "    The paths are stored as tuples of node names, ensuring a consistent identification system.\n",
    "    This approach allows for the identification of gateways even when only partial information about unique IDs is available,\n",
    "    as it relies on the structural signature of the paths rather than solely on the IDs.\n",
    "    The output is a sorted dictionary where each key represents a unique path structure (as a JSON string),\n",
    "    and the value is a list of gateways sharing that structure. This forms the basis for consistent node identification across different BPMN models.\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries for storing paths by gateway and gateway structures.\n",
    "    paths_by_gateway = {}\n",
    "    structure_to_gateways = {}\n",
    "\n",
    "    # Define a depth-first search (DFS) function to traverse the graph.\n",
    "    def dfs(current_node, current_path, visited_gateways):\n",
    "        # Check if current node is an Event or Task, marking the end of a path.\n",
    "        if isinstance(current_node, (BPMN.Event, BPMN.Task)):\n",
    "            current_path_tuple = tuple(current_path)\n",
    "            paths_by_gateway[start_gateway_id].append(current_path_tuple)\n",
    "            return\n",
    "\n",
    "        # Iterate over outgoing arcs from the current node.\n",
    "        for arc in current_node.get_out_arcs():\n",
    "            next_node = arc.get_target()\n",
    "            # Process next node if it is a Gateway, Event, or Task and not already visited.\n",
    "            if isinstance(next_node, (BPMN.Gateway, BPMN.Event, BPMN.Task)) and next_node not in visited_gateways:\n",
    "                # Get node name, using type name for gateways and node name otherwise.\n",
    "                node_name = type(next_node).__name__ if isinstance(next_node, BPMN.Gateway) else next_node.get_name()\n",
    "                current_path.append(node_name)\n",
    "                # Mark gateway as visited.\n",
    "                if isinstance(next_node, BPMN.Gateway):\n",
    "                    visited_gateways.add(next_node)\n",
    "                # Recursively call DFS on the next node.\n",
    "                dfs(next_node, current_path, visited_gateways)\n",
    "                # Backtrack by removing the last node from the path and unmarking the gateway.\n",
    "                current_path.pop()\n",
    "                if isinstance(next_node, BPMN.Gateway):\n",
    "                    visited_gateways.remove(next_node)\n",
    "\n",
    "    # Start DFS for each gateway node in the BPMN model.\n",
    "    for node in bpmn_graph.get_nodes():\n",
    "        if isinstance(node, BPMN.Gateway):\n",
    "            start_gateway_id = node.id\n",
    "            paths_by_gateway[start_gateway_id] = []\n",
    "            dfs(node, [f\"{type(node).__name__}_Source\"], set())\n",
    "\n",
    "    # Group and sort gateways based on their path structures.\n",
    "    for gateway, paths in paths_by_gateway.items():\n",
    "        # Sort each path\n",
    "        sorted_paths = sorted(paths)\n",
    "        # Convert each sorted path to a string\n",
    "        paths_strings = ['; '.join(path) for path in sorted_paths]\n",
    "        # Convert sorted list of path strings to a JSON string\n",
    "        paths_key = json.dumps(paths_strings)\n",
    "        structure_to_gateways.setdefault(paths_key, []).append(gateway)   \n",
    "    \n",
    "    sorted_structure_to_gateways = {k: structure_to_gateways[k] for k in sorted(structure_to_gateways)}\n",
    "\n",
    "    return sorted_structure_to_gateways\n",
    "            \n",
    "def handle_gateways_for_model(gateway_paths):\n",
    "    # Mapping for new names of gateways.\n",
    "    name_mappings = {}\n",
    "    \n",
    "    # Assign new names to gateways based on their path structure.\n",
    "    for path_structure, gateways in gateway_paths.items():\n",
    "        # Generate a hash value for the path structure string.\n",
    "        path_hash = hash(path_structure)\n",
    "\n",
    "        # Ensure the hash value is non-negative and convert it to a string.\n",
    "        path_hash_str = str(abs(path_hash))\n",
    "\n",
    "        for gateway in gateways:\n",
    "            new_name = f\"Gateway_{path_hash_str}\"\n",
    "            name_mappings[gateway] = new_name\n",
    "\n",
    "    return name_mappings\n",
    "\n",
    "def rebuild_bpmn_graph(bpmn_graph, gateway_mapping):\n",
    "    # Create a new BPMN model with the same name.\n",
    "    new_bpmn_graph = BPMN(name=bpmn_graph.get_name())\n",
    "    node_mapping = {}\n",
    "\n",
    "    # Add nodes to the new model, applying the gateway mapping.\n",
    "    sorted_nodes = sorted(bpmn_graph.get_nodes(), key=lambda node: node.get_name())\n",
    "    for node in sorted_nodes:\n",
    "        new_id = gateway_mapping.get(node.get_id(), node.get_name())\n",
    "        new_node = type(node)(id=new_id, name=new_id, process=node.get_process())\n",
    "        new_bpmn_graph.add_node(new_node)\n",
    "        node_mapping[node] = new_node\n",
    "\n",
    "    # Add flows to the new model, mapping source and target nodes.\n",
    "    sorted_flows = sorted(bpmn_graph.get_flows(), key=lambda flow: (flow.get_source().get_name(), flow.get_target().get_name()))\n",
    "    for flow in sorted_flows:\n",
    "        new_source = node_mapping[flow.get_source()]\n",
    "        new_target = node_mapping[flow.get_target()]\n",
    "        new_flow = type(flow)(source=new_source, target=new_target, id=flow.get_id(), name=flow.get_name(), process=flow.get_process())\n",
    "        new_bpmn_graph.add_flow(new_flow)\n",
    "        \n",
    "    return new_bpmn_graph\n",
    "\n",
    "# Main processing function\n",
    "def process_bpmn_graph(bpmn_graph):\n",
    "     # Extract and sort gateways to establish unique IDs.\n",
    "    gateway_paths = extract_gateways(bpmn_graph)\n",
    "\n",
    "    # Generate new names for gateways based on their path structures.\n",
    "    name_mappings = handle_gateways_for_model(gateway_paths)\n",
    "\n",
    "    # Rebuild the BPMN model with new gateway names.\n",
    "    new_bpmn_graph = rebuild_bpmn_graph(bpmn_graph, name_mappings)\n",
    "    \n",
    "    return new_bpmn_graph\n",
    "\n",
    "\n",
    "for mode in ProcessingMode:\n",
    "    for service in process_graph[mode.name]:\n",
    "        for subprocess in process_graph[mode.name][service]:\n",
    "            bpmn_graph = process_graph[mode.name][service][subprocess]['bpmn_graph']\n",
    "            process_graph[mode.name][service][subprocess]['bpmn_graph'] = process_bpmn_graph(bpmn_graph)\n",
    "print(\"Done\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e08d40-b1af-43c1-82ef-6a96013f2f30",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b06a2abe-2e00-440b-af66-e12c4618afce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:07.968193Z",
     "start_time": "2023-11-08T23:50:07.955523Z"
    }
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def heuristic(graph: nx.DiGraph, source: str, target: str) -> float:\n",
    "    return 0 # We dont have a grid\n",
    "\n",
    "def a_star_search(graph: nx.DiGraph, start: str, goal: str, logs: list, require_loop=False):\n",
    "    \"\"\"\n",
    "    Modified A* Search Algorithm to find loops.\n",
    "    \n",
    "    :param graph: NetworkX directed graph\n",
    "    :param start: Start node ID\n",
    "    :param goal: Target node ID\n",
    "    :param logs: List of logs with logging_statement_id\n",
    "    :return: Shortest path from start to goal, including loops if required\n",
    "    \"\"\"\n",
    "    log_names = [log['logging_statement_id'] for log in logs]  # Extract log names from logs\n",
    "    open_list = [(0, start, [])]  # Initialize open list with start node\n",
    "    g_score = {start: 0}  # Initialize g_score for start node as 0\n",
    "    \n",
    "    while open_list:  # Loop until open list is empty\n",
    "        f_score, current, path = heapq.heappop(open_list)  # Pop node with lowest f_score\n",
    "        \n",
    "        # If current node is the goal, and loop requirement is met, return path\n",
    "        if current == goal:\n",
    "            if not require_loop or (require_loop and len(path) > 1):\n",
    "                return path + [current]\n",
    "        \n",
    "        for neighbor, edge_data in graph[current].items():  # Loop through neighbors\n",
    "            # Skip nodes in logs except the goal\n",
    "            if neighbor in log_names and neighbor != goal:\n",
    "                continue\n",
    "            \n",
    "            # Calculate tentative g_score for neighbor\n",
    "            tentative_g_score = g_score[current] + edge_data.get('weight', 1)\n",
    "            \n",
    "            # Update g_score if new path is better or neighbor is not in open list\n",
    "            if tentative_g_score < g_score.get(neighbor, float('inf')) or neighbor not in [i[1] for i in open_list]:\n",
    "                g_score[neighbor] = tentative_g_score\n",
    "                f_score = tentative_g_score + heuristic(graph, neighbor, goal)\n",
    "                heapq.heappush(open_list, (f_score, neighbor, path + [current]))\n",
    "                \n",
    "    return None  # Return None if path is not found\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "017090d8-d6ef-46bf-b00c-44098eb5cc3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:50:08.011181Z",
     "start_time": "2023-11-08T23:50:07.984015Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def initialize_graph_attributes(graph: nx.DiGraph):\n",
    "    for node in graph.nodes():\n",
    "        # Initialize node attributes using the graph object\n",
    "        if f'count' not in graph.nodes[node]:\n",
    "            graph.nodes[node][f'count'] = 0\n",
    "\n",
    "    for u, v in graph.edges():\n",
    "        # Initialize edge attributes using the graph object\n",
    "        if f'count' not in graph[u][v]:\n",
    "            graph[u][v][f'count'] = 0\n",
    "\n",
    "def update_path_attributes(graph, path):\n",
    "    \"\"\"\n",
    "    Update attributes of edges in the given path.\n",
    "\n",
    "    :param graph: NetworkX graph\n",
    "    :param path: List of nodes forming a path\n",
    "    \"\"\"\n",
    "    for i in range(len(path) - 1):\n",
    "        edge = (path[i], path[i + 1])\n",
    "        \n",
    "        # Initialize count for the edge if not present\n",
    "        if f'count' not in graph.edges[edge]:\n",
    "            graph.edges[edge][f'count'] = 0\n",
    "            \n",
    "        # Increment the count for the edge\n",
    "        graph.edges[edge][f'count'] += 1\n",
    "    \n",
    "    # Update node attributes\n",
    "    for i, node in enumerate(path[1:-1]):  # Exclude the start and end nodes\n",
    "        # Initialize count for the node if not present\n",
    "        if f'count' not in graph.nodes[node]:\n",
    "            graph.nodes[node][f'count'] = 0\n",
    "            \n",
    "        # Increment the count for the node\n",
    "        graph.nodes[node][f'count'] += 1\n",
    "\n",
    "            \n",
    "def update_graph_from_logs(graph: nx.DiGraph, logs: list):\n",
    "    \"\"\"\n",
    "    Updates graph attributes based on logs\n",
    "    \"\"\"\n",
    "    initialize_graph_attributes(graph)\n",
    "    \n",
    "    # Step 1: Count occurrences of each logging_statement_id in logs\n",
    "    log_count = defaultdict(int)\n",
    "    for log in logs:\n",
    "        log_count[log['logging_statement_id']] += 1\n",
    "        \n",
    "    for i in range(len(logs) - 1):\n",
    "        start_node = logs[i]['logging_statement_id']\n",
    "        end_node = logs[i + 1]['logging_statement_id']\n",
    "        \n",
    "        require_loop = start_node == end_node \n",
    "        \n",
    "        if start_node not in graph or end_node not in graph:\n",
    "            missing_type = []\n",
    "            if start_node not in graph:\n",
    "                missing_type.append(f\"Start node: {start_node}\")\n",
    "            if end_node not in graph:\n",
    "                missing_type.append(f\"End node: {end_node}\")\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        # A* search to find path between start_node and end_node\n",
    "        path = a_star_search(graph, start_node, end_node, logs, require_loop)\n",
    "        if path:\n",
    "            update_path_attributes(graph, path)                   \n",
    "        # else:\n",
    "        #     # Print a warning if the path is not found\n",
    "        #     display(HTML(f\"<b><span style='color: orange;'>&emsp;&emsp; --> Path not found between {start_node} and {end_node} in Model {model} - Require Loop {require_loop}.</span></b>\"))\n",
    "            \n",
    "            # display(HTML(f\"<b><span style='color: orange;'>&emsp;&emsp; Attempting to add bridge...</span></b>\"))\n",
    "             # Add a dashed edge with isBridge=True attribute\n",
    "#             graph.add_edge(start_node, end_node, style='dashed', isBridge=True)\n",
    "            # plot_graph_with_missing_path(graph, start_node, end_node, title=f\"Missing path {start_node} to {end_node} in Model {model}\")\n",
    "            \n",
    "#             new_path = a_star_search(graph, start_node, end_node)\n",
    "#             if new_path:\n",
    "#                 # Update the path attributes using the new path\n",
    "#                 update_path_attributes(graph, new_path, model)\n",
    "#             else:\n",
    "#                 display(HTML(f\"<b><span style='color: red;'>&emsp;&emsp; --> Path not found between {start_node} and {end_node} in Model {model}</span></b>\"))\n",
    "\n",
    "    # Set each node's count that corresponds to a log to that log count\n",
    "    for node, count in log_count.items():\n",
    "        if node in graph:\n",
    "            graph.nodes[node][f'count'] = count\n",
    "\n",
    "    # # Step 2: Validate counts\n",
    "    # for node, attr in graph.nodes(data=True):\n",
    "    #     if attr.get('type') == 'transition':\n",
    "    #         if f'count_{model}' in attr:\n",
    "    #             if attr[f'count_{model}'] != log_count.get(node, 0):\n",
    "    #                 display(HTML(f\"<b><span style='color: red;'>&emsp;&emsp; --> Mismatch in counts for node {node} in Model {model}. Count in graph: {attr[f'count_{model}']}, Count in logs: {log_count.get(node, 0)}</span></b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1708d-3bc7-491b-9671-de9ff66a1505",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff4e8d81-d782-421d-b453-c86edb6b9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_agraph import to_agraph\n",
    "\n",
    "def plot_graph(graph, title):\n",
    "\n",
    "    \n",
    "    A = to_agraph(graph)\n",
    "    \n",
    "    for node in A.nodes():\n",
    "        node_name = node.name\n",
    "        node_type = graph.nodes[node_name].get('type', None)\n",
    "        \n",
    "        if node_type == 'StartEvent':\n",
    "            node.attr['shape'] = 'circle'\n",
    "            node.attr['label'] = ''\n",
    "            node.attr['width'] = 0.6\n",
    "            node.attr['height'] = 0.6\n",
    "            node.attr['fillcolor'] = 'green'\n",
    "            node.attr['style'] = 'filled'    \n",
    "            \n",
    "        elif node_type == 'NormalEndEvent':\n",
    "            node.attr['shape'] = 'circle'\n",
    "            node.attr['label'] = ''\n",
    "            node.attr['width'] = 0.6\n",
    "            node.attr['height'] = 0.6\n",
    "            node.attr['fillcolor'] = 'orange'\n",
    "            node.attr['style'] = 'filled'      \n",
    "            \n",
    "        elif node_type == 'Task':\n",
    "            node.attr['shape'] = 'rectangle'\n",
    "          \n",
    "        elif node_type == 'ExclusiveGateway':\n",
    "            node.attr['shape'] = 'diamond'\n",
    "            node.attr['label'] = 'X'\n",
    "            node.attr['width'] = 0.6\n",
    "            node.attr['height'] = 0.6\n",
    "            node.attr['style'] = 'filled'\n",
    "\n",
    "        elif node_type == 'ParallelGateway':\n",
    "            node.attr['shape'] = 'diamond'\n",
    "            node.attr['label'] = '+'\n",
    "            node.attr['width'] = 0.6\n",
    "            node.attr['height'] = 0.6\n",
    "            node.attr['style'] = 'filled'  \n",
    "        \n",
    "        else :\n",
    "            print(f\"Could not handle node of type: {node_type}\")\n",
    "            \n",
    "            \n",
    "    A.layout(prog='dot')\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    A.draw(f\"Graphs/Single/{title}.png\")\n",
    "    img = plt.imread(f\"Graphs/Single/{title}.png\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d80295a6-ab22-4933-b495-2ceb174bdc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../Data/Reproduced Experiment/Diagnosis/Models/Part A/model_partA.json\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from networkx.drawing.nx_agraph import to_agraph, write_dot\n",
    "from pm4py.objects.petri_net.obj import PetriNet\n",
    "from pm4py.objects.petri_net.utils.networkx_graph import create_networkx_directed_graph\n",
    "from pm4py.objects.petri_net.utils import petri_utils\n",
    "\n",
    "def print_networkx_graph_structure(G):\n",
    "    \"\"\"\n",
    "    Print the structure of a NetworkX graph.\n",
    "    \"\"\"\n",
    "    print(\"Nodes in the Graph:\")\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        print(f\"Node: {node}, Attributes: {attrs}\")\n",
    "\n",
    "    print(\"\\nEdges in the Graph:\")\n",
    "    for edge in G.edges():\n",
    "        print(f\"Edge from {edge[0]} to {edge[1]}\")\n",
    "        \n",
    "def bpmn_to_networkx(bpmn_model):\n",
    "    \"\"\"\n",
    "    Convert a BPMN model to a NetworkX graph.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes to the NetworkX graph using names\n",
    "    for node in bpmn_model.get_nodes():\n",
    "        G.add_node(node.name, label=node.name, type=type(node).__name__)\n",
    "\n",
    "    # Add edges to the NetworkX graph using names\n",
    "    for flow in bpmn_model.get_flows():\n",
    "        source_name = flow.source.name if hasattr(flow.source, 'name') else None\n",
    "        target_name = flow.target.name if hasattr(flow.target, 'name') else None\n",
    "        if source_name and target_name:\n",
    "            G.add_edge(source_name, target_name)\n",
    "\n",
    "    return G\n",
    "\n",
    "def annotate_single_graph(G):\n",
    "    \"\"\"\n",
    "    Annotate the single graph nodes and edges based on their attributes and log data.\n",
    "    \"\"\"\n",
    "    # Annotate nodes\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node].setdefault('count', 0)\n",
    "        G.nodes[node].setdefault('in_graph', True)\n",
    "\n",
    "    # Annotate edges\n",
    "    for u, v, attributes in G.edges(data=True):\n",
    "        G.edges[(u, v)].setdefault('count', 0)\n",
    "        G.edges[(u, v)].setdefault('in_graph', True)\n",
    "\n",
    "        \n",
    "def save_and_plot_single_model(process_graph, raw_log_data):\n",
    "    \"\"\"\n",
    "    Saves and plots all models based on raw log data for a single model.\n",
    "    \"\"\"\n",
    "    json_data = {}\n",
    "    \n",
    "    service_names = []\n",
    "    \n",
    "    for mode_name, services in process_graph.items():\n",
    "        \n",
    "        json_data[mode_name] = {\n",
    "        \"viewType\": \"Single\", \n",
    "        \"graphType\": \"BPMN\", \n",
    "        \"data\": {} \n",
    "        }\n",
    "        \n",
    "        for service_name, subprocesses in services.items():\n",
    "            \n",
    "            service_names.append(service_name)\n",
    "            \n",
    "            json_data[mode_name][\"data\"][service_name] = {}\n",
    "            \n",
    "            for subprocess_name, model_data in subprocesses.items():\n",
    "                bpmn_graph = model_data.get('bpmn_graph', None)\n",
    "                if bpmn_graph is None:\n",
    "                    continue\n",
    "\n",
    "                G = bpmn_to_networkx(bpmn_graph)\n",
    "                log_data = raw_log_data.get(mode_name, {}).get(service_name, {}).get(subprocess_name, {})   \n",
    "                annotate_single_graph(G)\n",
    "                update_graph_from_logs(G, log_data.get('logs', []))\n",
    "                title = f\"{service_name} - {subprocess_name}\"\n",
    "                # print_networkx_graph_structure(G)\n",
    "                # plot_graph(G, title)\n",
    "\n",
    "                temp_dot_file = f\"temp_graph.dot\"\n",
    "                write_dot(G, temp_dot_file)\n",
    "                with open(temp_dot_file, 'r') as file:\n",
    "                    graph_content = file.read()\n",
    "                os.remove(temp_dot_file)  \n",
    "                \n",
    "                json_data[mode_name][\"data\"][service_name][subprocess_name] = {\n",
    "                    \"graphData\": graph_content,\n",
    "                    \"logData\": log_data\n",
    "                }\n",
    "                \n",
    "        \n",
    "        # Add Blackbox Services\n",
    "        service_names.append(\"web-app\")\n",
    "        service_names.append(\"auth-service\")\n",
    "\n",
    "        json_data[mode_name][\"services\"] = service_names\n",
    "            \n",
    "        save_json(json_data[mode_name], f\"../Data/Reproduced Experiment/Diagnosis/Models/Part A/model_partA.json\")\n",
    "        \n",
    "def save_json(data, file_path):\n",
    "    \"\"\"Save a Python dictionary to a JSON file.\"\"\"\n",
    "    print(f\"Saving {file_path}\")\n",
    "    \n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        \n",
    "        \n",
    "# Call the function to save and plot all models\n",
    "save_and_plot_single_model(process_graph, logs)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7bd87f-589b-4bd7-af18-47bb80d8f0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
